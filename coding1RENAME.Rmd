# Coding Assignment 1
### Nishanth Alladi (nalladi2), Ethan Cook (elcook3), Davy Ji (davyji2)

### Question 1
```{r}

set.seed(39482342)

csize = 10       # number of centers
p = 2     # number of dimensions    
s = 1        # sd for generating the centers within each class                    
m1 = matrix(rnorm(csize*p), csize, p)*s + cbind( rep(1,csize), rep(0,csize) ) # m1 = all center data for class 1
m0 = matrix(rnorm(csize*p), csize, p)*s + cbind( rep(0,csize), rep(1,csize) ) # m0 = all center data for class 2

train_n = 100

# Allocate the n samples for class 1  to the 10 clusters
id1 = sample(1:csize, train_n, replace=TRUE)
# Allocate the n samples for class 1 to the 10 clusters
id0 = sample(1:csize, train_n, replace=TRUE)  
s = sqrt(1/5);              # sd for generating data. 

traindata = matrix(rnorm(2*train_n*p), 2*train_n, p)*s + 
  rbind(m1[id1,], m0[id0,])
dim(traindata)
Ytrain = c(rep(TRUE,train_n), rep(FALSE,train_n))

# FALSE is class 0 and TRUE is class 1

test_n = 5000
# Allocate the n samples for class 1  to the 10 clusters
id1 = sample(1:csize, test_n, replace=TRUE)
# Allocate the n samples for class 1 to the 10 clusters
id0 = sample(1:csize, test_n, replace=TRUE)  

ourtestdata = matrix(rnorm(2*test_n*p), 2*test_n, p)*s + 
  rbind(m1[id1,], m0[id0,])
dim(traindata)
Ytest = c(rep(TRUE,test_n), rep(FALSE,test_n))
```

```{r}
n = 100
plot(traindata[,1], traindata[,2], type="n", xlab="", ylab="")
points(traindata[1:n,1], traindata[1:n,2], col="blue")
points(traindata[(n+1):(2*n),1], traindata[(n+1):(2*n),2], col="red")
points(m1[1:csize,1], m1[1:csize,2], pch="+", cex=1.5, col="blue")
points(m0[1:csize,1], m0[1:csize,2], pch="+", cex=1.5, col="red");   
legend("bottomright", pch = c(1,1), col = c("red", "blue"), legend = c("class 1", "class 0"))
```

```{r}
# kNN function
knn_homemade = function(traindata, ourtestdata, k) {
  outer_subtraction = outer(ourtestdata, traindata, FUN = "-")
  distances = sqrt(outer_subtraction[,1,,1]^2 + outer_subtraction[,2,,2]^2)
  sorted_distances_and_indicies = apply(distances, 1, function(x) sort(x, index.return = TRUE))
  closest_k = lapply(1: length(sorted_distances_and_indicies), function (j) sorted_distances_and_indicies[[j]]$ix[1:k] )
  class_distinction = lapply(closest_k, function(x) x < 100)
  unlist(lapply(class_distinction, function(x) ifelse(sum(x) == k/2, rnorm(1) > 0, sum(x) > k/2)))
}
```

```{r}
results = knn_homemade(traindata, ourtestdata, 5)
```

```{r}
confusion_matrix = function(A, B) {
  N = length(A)
  TP = sum((A == B) & A) / N
  TN = sum((A == B) & !A) / N
  FP = sum((A != B) & A) / N
  FN = sum((A != B) & !A) / N
  rbind(c(TP, FP), c(FN, TN))
}
```

```{r}
library(class)
testknn = function(k) {
  cat("k = ", k, "\n")
  cat("our knn \n")
  our_output = knn_homemade(traindata, ourtestdata, k)
  print(matrix(c(c("TP", "FN"), c("FP", "TN")), 2, 2))
  print(confusion_matrix(our_output, Ytest))
  cat("\n")
  
  cat("R's knn \n")
  r_output = knn(traindata, ourtestdata, Ytrain, k)
  levels(r_output) = c(FALSE, TRUE)
  r_output = as.logical(r_output)
  print(matrix(c(c("TP", "FN"), c("FP", "TN")), 2, 2))
  print(confusion_matrix(r_output, Ytest))
  cat("\n")
}
```

```{r}
for (k in c(1, 3, 5)){
  testknn(k)
}
```

```{r}
load("Dan_toydata.RData")

out = knn(traindata, testdata, Ytrain, prob=TRUE, k=5)
attr(out, "prob") # retrieve prob vector
```


```{r}
ks = 1:180
k_folds = 10

data = traindata
N = nrow(data)
randomized_indices = sample(1:N)
buckets = split(randomized_indices, ceiling(seq_along(randomized_indices) / (N / k_folds))) 

best_k = 0
best_score = 0
for (k in ks) {
  correct_predictions = 0
  for (i in 1:k_folds) {
    test_set = traindata[unlist(buckets[i], use.names = FALSE),]
    test_indices = unlist(buckets[i], use.names = FALSE) < 100
    training_set = traindata[unlist(buckets[-i], use.names = FALSE),]
    knn_results = knn_homemade(training_set, test_set, k)
    correct_predictions = sum(test_indices == knn_results) + correct_predictions
  }
  if (correct_predictions >= best_score) {
    best_k = k
    best_score = correct_predictions
  }
}

print(best_k)
output = knn_homemade(traindata, ourtestdata, best_k)
print(matrix(c(c("TP", "FN"), c("FP", "TN")), 2, 2))
print(confusion_matrix(output, Ytest))
```
```{r}
class1 = ourtestdata[1:test_n,]
class0 = ourtestdata[(test_n+1):(test_n*2),]
numerators = 0
denominators = 0
for (l in 1:10) {
  numerators = numerators + exp(-rowSums( (ourtestdata - mean(class1[id1 == l])) ^ 2 ) / (2*(s^2)))
  denominators = denominators + exp(-rowSums( (ourtestdata - mean(class0[id0 == l])) ^ 2 ) / (2*(s^2)))
}
predictions = (numerators / denominators) >= 1

print(matrix(c(c("TP", "FN"), c("FP", "TN")), 2, 2))
print(confusion_matrix(predictions, Ytest))
```

