```{r}
library("stopwords")
library(text2vec)
library("pROC")
library(glmnet)
library(data.table)
library(magrittr)
```

```{r}
train = read.table("split_1/train.tsv",
                   stringsAsFactors = FALSE,
                   header = TRUE)
test = read.table("split_1/test.tsv",
                   stringsAsFactors = FALSE,
                   header = TRUE)
train$review = gsub('&lt;.*?&gt;', ' ', train$review)
test$review = gsub('&lt;.*?&gt;', ' ', test$review)
test_y  = read.table("split_1/test_y.tsv",
                   stringsAsFactors = FALSE,
                   header = TRUE)
```


```{r}
stop_words = stopwords::stopwords("en", source = "nltk")
it_train = itoken(train$review,
                  preprocessor = tolower, 
                  tokenizer = word_tokenizer)
tmp.vocab = create_vocabulary(it_train, 
                              stopwords = stop_words, 
                              ngram = c(1L,4L))
tmp.vocab = prune_vocabulary(tmp.vocab, term_count_min = 10,
                             doc_proportion_max = 0.5,
                             doc_proportion_min = 0.001)
dtm_train  = create_dtm(it_train, vocab_vectorizer(tmp.vocab))
```

```{r}
#Ridge to select vocab TBD:
# myvocab = tmp.vocab$term
```

```{r}
pos_dtm = dtm_train[train[['sentiment']] == 1,]
neg_dtm = dtm_train[train[['sentiment']] == 0,]
top_2k_t = c()
top_2k_i = c()
for (col in 1:ncol(dtm_train)) {
  X = pos_dtm[,col]
  Y = neg_dtm[,col]
  t = t.test(X, Y)$statistic
  if (length(top_2k_i) < 1000) {
    top_2k_i = c(top_2k_i, col)
    top_2k_t = c(top_2k_t, t)
  } else {
    t2k_min = min(unlist(top_2k_t))
    if (abs(t) > t2k_min) {
      idx = which.min(top_2k_t)
      top_2k_i[idx] = col
      top_2k_t[idx] = abs(t)
    }
  }
}
```

```{r}
myvocab = colnames(dtm_train)[top_2k_i]
```

```{r}
#"using this customized vocabulary, I proceeded with ridge regression on the five data splits. For each split, the initial part of my code resembled the following"
vectorizer = vocab_vectorizer(create_vocabulary(myvocab, 
                                                  ngram = c(1L, 2L)))
dtm_train_new = create_dtm(it_train, vectorizer)
```

```{r}
# do the iterative lasso 
# for (i in 1:50) {
#   n = nrow(dtm_train_new)
#   subsetidxs = sample(1:n, .6*n)
#   subset_glm = cv.glmnet(x = dtm_train_new[subsetidxs,], 
#                          y = train[['sentiment']][subsetidxs],
#                          family = "binomial")
#   
# }

# glm = cv.glmnet(x = dtm_train_new, 
#                        y = train[['sentiment']],
#                        family = "binomial")
# betacols = colnames(glm$glmnet.fit$beta)
# betas = sort(abs(glm$glmnet.fit$beta[,betacols[length(betacols)]]), decreasing=TRUE)
# myvocab = names(betas[1:999])
```

```{r}
# vectorizer = vocab_vectorizer(create_vocabulary(myvocab, 
#                                                   ngram = c(1L, 2L)))
# dtm_train_new = create_dtm(it_train, vectorizer)
```

```{r}
NFOLDS = 4
glmnet_classifier = cv.glmnet(x = dtm_train_new, y = train[['sentiment']],
                              family = 'binomial',
                              # L1 penalty
                              alpha = 0,
                              # interested in the area under ROC curve
                              type.measure = "auc",
                              # 5-fold cross-validation
                              nfolds = NFOLDS,
                              # high value is less accurate, but has faster training
                              thresh = 1e-3,
                              # again lower number of iterations for faster training
                              maxit = 1e3)
```


```{r}
it_test = word_tokenizer(tolower(test$review))
it_test = itoken(it_test, ids = test$id,
                 # turn off progressbar because it won't look nice in rmd
                 progressbar = FALSE)

dtm_test = create_dtm(it_test, vectorizer)

preds = predict(glmnet_classifier, dtm_test, type = 'response')[,1]
glmnet:::auc(test_y$sentiment, preds)
```