```{r}
library("stopwords")
library(text2vec)
library("pROC")
library(glmnet)
library(data.table)
library(magrittr)
```

## How we constructed our vocabulary to be 1000 words

First we read in train and test data, and then combine them to be one dataset, as permitted in Campuswire post #626. 

```{r}
train = read.table("split_1/train.tsv",
                   stringsAsFactors = FALSE,
                   header = TRUE)
test = read.table("split_1/test.tsv",
                   stringsAsFactors = FALSE,
                   header = TRUE)
train$review = gsub('&lt;.*?&gt;', ' ', train$review)
test$review = gsub('&lt;.*?&gt;', ' ', test$review)
all_reviews = c(train$review, test$review)

test_y  = read.table("split_1/test_y.tsv",
                   stringsAsFactors = FALSE,
                   header = TRUE)
all_sentiments = c(train[['sentiment']], test_y$sentiment)
```

After this, we build a document term matrix of all the n_grams of size 1 and 2 from the train and test data, with stopwords from the stopwords R package. 

```{r}
stop_words = stopwords::stopwords("en", source = "nltk")
it_train = itoken(all_reviews,
                  preprocessor = tolower,
                  tokenizer = word_tokenizer)
it_train2 = itoken(train$review,
                  preprocessor = tolower,
                  tokenizer = word_tokenizer)

tmp.vocab = create_vocabulary(it_train, 
                              stopwords = stop_words, 
                              ngram = c(1L,2L))
tmp.vocab = prune_vocabulary(tmp.vocab, term_count_min = 10,
                             doc_proportion_max = 0.5,
                             doc_proportion_min = 0.001)
dtm_train  = create_dtm(it_train, vocab_vectorizer(tmp.vocab))
```

Next, we carry out two sample t-tests for every "word" in our document term matrix, by comparing the number of times word appeared in positive reviews and in negative reviews. We record the 1000 "words" with the highest absolute t statistics (which indicates that a difference in means is more likely) and have that as our vocab.

```{r}
pos_dtm = dtm_train[all_sentiments == 1,]
neg_dtm = dtm_train[all_sentiments == 0,]
top_2k_t = c()
top_2k_i = c()
for (col in 1:ncol(dtm_train)) {
  X = pos_dtm[,col]
  Y = neg_dtm[,col]
  t = t.test(X, Y)$statistic
  if (length(top_2k_i) < 999) {
    top_2k_i = c(top_2k_i, col)
    top_2k_t = c(top_2k_t, t)
  } else {
    t2k_min = min(unlist(top_2k_t))
    if (abs(t) > t2k_min) {
      idx = which.min(top_2k_t)
      top_2k_i[idx] = col
      top_2k_t[idx] = abs(t)
    }
  }
}
```

Next we write to a file

```{r}
myvocab = colnames(dtm_train)[top_2k_i]
write(myvocab, "myvocab.txt")
```
